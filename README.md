# ğŸš€ RSS + LLM Pipeline - Version 3.0 Architecture Hybride

> **Pipeline RSS automatisÃ©** avec Intelligence Artificielle + Fallback Expert System  
> âœ… **100% portable** â€¢ ğŸ³ **Docker** â€¢ ğŸ¤– **LLM rÃ©els** â€¢ ğŸ§  **Fallback intelligent** â€¢ ğŸ“ **Auto-rÃ©sumÃ©s IA**

---

## âš¡ Usage Quotidien (30 secondes)

### ğŸŒ… **DÃ©marrage Rapide Quotidien**
```bash
# Windows - DÃ©marrage ultra-rapide des conteneurs existants
start_containers.bat
```
> **OptimisÃ© quotidien** : DÃ©marre directement les conteneurs sans rebuild  
> **Temps** : ~15 secondes â€¢ **Auto-ouverture** Node-RED â€¢ **LLM chargÃ©s** automatiquement

### ğŸŒ™ **ArrÃªt Propre**
```bash
# Windows - ArrÃªt propre des conteneurs
stop_containers.bat
```

### ğŸ”§ **Scripts Disponibles**
| Script | Usage | Description |
|--------|-------|-------------|
| **`start_containers.bat`** | ğŸŒ… **Quotidien** | DÃ©marre conteneurs + LLM (ultra-rapide) |
| **`stop_containers.bat`** | ğŸŒ™ **Quotidien** | ArrÃªte proprement conteneurs + LLM |
| `deployment/start_pipeline.bat` | ğŸ”§ Installation | Premier dÃ©marrage avec build + tÃ©lÃ©chargement modÃ¨les |
| `deployment/stop_pipeline.bat` | ğŸ›‘ Maintenance | ArrÃªt docker-compose complet |

> **ğŸ’¡ Astuce** : Les modÃ¨les LLM sont mis en cache - le dÃ©marrage quotidien reste rapide mÃªme avec l'IA

---

## ğŸ¯ Installation Initiale (10 minutes)

### ğŸ“‹ PrÃ©requis
- **Docker Desktop** (Windows/Mac/Linux)
- **12+ Go RAM** disponible (LLM + conteneurs)
- **20+ Go stockage** libre (modÃ¨les LLM + cache)
- **Connexion internet** (tÃ©lÃ©chargement modÃ¨les initial)

### âš¡ PremiÃ¨re Installation
```bash
# 1. Cloner le projet
git clone https://github.com/RostamFraudi/RSS_LLM_Pipeline_Stable.git
cd RSS_LLM_Pipeline_Stable

# 2. Premier dÃ©marrage avec build + modÃ¨les LLM (Windows)
deployment\start_pipeline.bat

# 3. Premier dÃ©marrage avec build + modÃ¨les LLM (Linux/Mac)
chmod +x deployment/start_pipeline.sh && ./deployment/start_pipeline.sh
```

### ğŸ¤– **TÃ©lÃ©chargement Automatique des ModÃ¨les**
Au premier dÃ©marrage, le systÃ¨me tÃ©lÃ©charge automatiquement :
- **DeBERTa-v3-base-mnli** : Classification zero-shot (420 Mo)
- **distilbart-cnn-12-6** : GÃ©nÃ©ration de rÃ©sumÃ©s (760 Mo)

> **â±ï¸ Note** : Premier dÃ©marrage ~5-10 min (tÃ©lÃ©chargement), puis 15s quotidien

### ğŸ”„ **AprÃ¨s Installation - Usage Quotidien**
```bash
# âœ… Utilisez les scripts rapides Ã  la racine :
start_containers.bat    # ğŸŒ… Matin (15s) + LLM chargÃ©s
stop_containers.bat     # ğŸŒ™ Soir (5s) + LLM sauvegardÃ©s

# âŒ Ã‰vitez les scripts deployment (lents) :
# deployment\start_pipeline.bat  # Build complet + re-tÃ©lÃ©chargement
```

### ğŸŒ Validation
- **LLM Service + IA** : http://localhost:15000/status
- **Health Check** : http://localhost:15000/health
- **Node-RED** : http://localhost:18880 (ouvert automatiquement)
- **Articles IA** : `obsidian_vault/articles/`

---

## ğŸ§  Architecture Hybride LLM + Expert System

### ğŸ¯ **RÃ©volution v3.0 : IA RÃ©elle + Fallback Intelligent**
Cette version marque une **Ã©volution majeure** vers une vraie intelligence artificielle avec sÃ©curitÃ© :

- **ğŸ¤– Classification LLM** : Zero-shot avec DeBERTa-v3 (8 catÃ©gories de fraude)
- **ğŸ“ RÃ©sumÃ©s IA** : GÃ©nÃ©ration contextuelle avec distilbart-cnn
- **ğŸ›¡ï¸ Fallback Expert** : SystÃ¨me de rÃ¨gles si LLM indisponible
- **ğŸ·ï¸ Tags intelligents** : GÃ©nÃ©ration automatique en anglais (jusqu'Ã  6 tags)
- **ğŸš¨ Alertes contextuelles** : 4 niveaux (critical, urgent, watch, info)

### ğŸ”„ **Flux de Traitement Hybride**
```
ğŸ“¡ RSS â†’ ğŸ¤– LLM Classification â†’ ğŸ“ RÃ©sumÃ© IA â†’ ğŸ·ï¸ Tags Auto â†’ ğŸš¨ Alertes â†’ ğŸ“ Obsidian
         â†“ (si Ã©chec LLM)
         ğŸ§  Expert System â†’ ğŸ“‹ Template â†’ ğŸ·ï¸ Tags prÃ©dÃ©finis â†’ ğŸ“ Obsidian
```

### ğŸ“ Structure du Projet
```
RSS_LLM_Pipeline_Stable/
â”œâ”€â”€ ğŸš€ start_containers.bat      # â­ DÃ©marrage quotidien LLM (RAPIDE)
â”œâ”€â”€ ğŸ›‘ stop_containers.bat       # â­ ArrÃªt quotidien LLM (RAPIDE)
â”œâ”€â”€ ğŸ“ deployment/               # Scripts maintenance
â”‚   â”œâ”€â”€ start_pipeline.bat       # Premier dÃ©marrage + tÃ©lÃ©chargement modÃ¨les
â”‚   â”œâ”€â”€ test_pipeline.bat        # Tests LLM + Expert System
â”‚   â”œâ”€â”€ stop_pipeline.bat        # ArrÃªt complet + nettoyage cache
â”‚   â””â”€â”€ view_logs.bat            # Monitoring LLM + fallback
â”œâ”€â”€ ğŸ³ docker_service/           # Services containerisÃ©s + IA
â”‚   â”œâ”€â”€ docker-compose.yml       # Orchestration + volumes modÃ¨les LLM
â”‚   â”œâ”€â”€ llm_service/             # Service IA hybride (app.py)
â”‚   â”œâ”€â”€ nodered_custom/          # Pipeline RSS â†’ LLM API
â”‚   â””â”€â”€ scripts/                 # Utilitaires + MOC generator
â”œâ”€â”€ âš™ï¸ config/                  # Configuration IA + Expert
â”‚   â”œâ”€â”€ sources.json             # Domaines + sources + mots-clÃ©s fallback
â”‚   â””â”€â”€ prompts.json             # Prompts LLM optionnels
â”œâ”€â”€ ğŸ“ obsidian_vault/           # Sortie articles enrichis IA
â”‚   â””â”€â”€ articles/                # Articles par domaine + tags IA
â”œâ”€â”€ ğŸ”§ node_red_data/            # DonnÃ©es Node-RED + cache LLM
â””â”€â”€ ğŸ“š documentation/            # Guides IA + Expert System
```

### ğŸ¯ **Nouveaux Domaines Anti-Fraude v3.0**
| Domaine | SpÃ©cialitÃ© | Classification | Tags IA |
|---------|------------|----------------|---------|
| `fraude_investissement` | ğŸ’° Arnaques placement, Ponzi | LLM + 25 mots-clÃ©s | investment-fraud, scam-alert |
| `fraude_paiement` | ğŸ’³ Skimming, phishing bancaire | LLM + 31 mots-clÃ©s | payment-security, card-fraud |
| `fraude_president_cyber` | ğŸ­ FOVI, usurpation dirigeants | LLM + 31 mots-clÃ©s | ceo-fraud, wire-fraud |
| `fraude_ecommerce` | ğŸ›’ **NOUVEAU** E-commerce scams | LLM + rÃ¨gles | ecommerce-fraud, online-scam |
| `supply_chain_cyber` | ğŸ”— **NOUVEAU** Supply chain attacks | LLM + rÃ¨gles | supply-chain, third-party-risk |
| `fraude_crypto` | â‚¿ Rugpull, DeFi scams | LLM + 32 mots-clÃ©s | crypto-fraud, blockchain-scam |
| `cyber_investigations` | ğŸ” Forensic, threat intelligence | LLM + 29 mots-clÃ©s | cybercrime, investigation |
| `intelligence_economique` | ğŸ•µï¸ Veille stratÃ©gique | LLM + 27 mots-clÃ©s | business-intel, risk-analysis |

---

## ğŸ¤– Services & APIs v3.0

### ğŸ§  Service IA Hybride (Port 15000)

#### **Nouveaux Endpoints v3.0**
```bash
POST /generate_metadata    # ğŸ¯ **PRINCIPAL** : Classification LLM + Tags + Alertes
POST /classify             # ğŸ” Classification LLM avec fallback
POST /summarize            # ğŸ“ RÃ©sumÃ© IA adaptatif par domaine
GET  /status              # ğŸ“Š **NOUVEAU** : Statut dÃ©taillÃ© IA + modÃ¨les
GET  /health              # â¤ï¸ SantÃ© service
GET  /config/info         # ğŸ“‹ **NOUVEAU** : Configuration actuelle
POST /reload_config       # ğŸ”„ **NOUVEAU** : Rechargement Ã  chaud
```

#### **Endpoints CompatibilitÃ© v2**
```bash
POST /classify_v2         # Alias vers /generate_metadata
POST /summarize_v2        # Alias vers /summarize
```

#### **Classification IA Hybride en Action**
```bash
curl -X POST http://localhost:15000/generate_metadata \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Nouvelle arnaque aux investissements crypto avec IA",
    "content": "Des escrocs utilisent de fausses IA pour attirer les investisseurs...",
    "source": "AMF France"
  }'
```

**RÃ©ponse IA v3.0** :
```json
{
  "domain": "fraude_crypto",
  "domain_label": "Fraude aux Cryptomonnaies",
  "confidence": 94,
  "classification_method": "llm_zero_shot",
  "alert_level": "urgent",
  "tags": ["crypto-fraud", "ai-scam", "investment-fraud", "urgent"],
  "obsidian_concepts": ["Cryptocurrency", "Blockchain Security", "Digital Assets"],
  "output_folder": "fraude-crypto",
  "processing_time": 0.31,
  "llm_used": true,
  "version": "3.0_hybrid"
}
```

#### **Monitoring IA**
```bash
# Statut dÃ©taillÃ© des modÃ¨les LLM
curl http://localhost:15000/status

# Exemple de rÃ©ponse
{
  "service": "RSS LLM Service v3.0 - Hybrid Architecture",
  "models": {
    "classifier": "DeBERTa-v3-base-mnli",
    "summarizer": "distilbart-cnn-12-6", 
    "status": "operational"
  },
  "performance": {
    "classification_avg": "0.3s",
    "summary_avg": "1.2s"
  }
}
```

### ğŸ”§ Node-RED + IA (Port 18880)
- **Interface graphique** : Configuration flux IA + Expert
- **Pipeline hybride** : RSS â†’ LLM â†’ Fallback â†’ Obsidian
- **DÃ©clencheur intelligent** : Toutes les 30 minutes + dÃ©tection Ã©checs LLM
- **Monitoring IA** : Debug LLM temps rÃ©el, mÃ©triques performance

---

## ğŸ·ï¸ SystÃ¨me de Tags IA + Alertes

### ğŸ¤– **GÃ©nÃ©ration Automatique de Tags v3.0**
Le systÃ¨me IA gÃ©nÃ¨re intelligemment jusqu'Ã  **6 tags en anglais** :

- **Tags de base par domaine** : `investment-fraud`, `payment-security`, `ceo-fraud`
- **Tags de confiance** : `high-confidence` (>90%), `medium-confidence` (>70%)
- **Tags d'urgence** : `urgent`, `critical` (dÃ©tection contextuelle)
- **Tags techniques** : `ransomware`, `phishing`, `data-breach`, `zero-day`
- **Tags business** : `funding`, `business-event`, `ipo`

### ğŸš¨ **SystÃ¨me d'Alertes Contextuelles v3.0**
Classification automatique en **4 niveaux** :

| Niveau | CritÃ¨res | Exemples |
|--------|----------|----------|
| `critical` | Breach, attack, ransomware, zero-day | Cyberattaque majeure, vol de donnÃ©es |
| `urgent` | Warning, risk, threat, domaines sensibles | Nouvelle vulnÃ©rabilitÃ©, FOVI dÃ©tectÃ©e |
| `watch` | Confiance >80% + domaines financiers | AMF alerte, crypto scam Ã©mergent |
| `info` | Articles informatifs, veille gÃ©nÃ©rale | Mise Ã  jour rÃ©glementaire, analyse |

---

## ğŸ§ª Tests & Validation v3.0

### ğŸ” Tests AutomatisÃ©s IA
```bash
# Test complet pipeline LLM + fallback
deployment\test_pipeline.bat
```

### ğŸ“Š MÃ©triques QualitÃ© IA Hybride
- **Classification LLM** : 90-97% prÃ©cision (quand disponible)
- **Fallback Expert** : 85-90% prÃ©cision (si LLM indisponible)
- **Performance IA** : <1s par article (classification + rÃ©sumÃ©)
- **DisponibilitÃ© hybride** : >99% (LLM + fallback)
- **Tags IA** : 4-6 tags pertinents par article
- **Alertes contextuelles** : PrÃ©cision >95% (critical/urgent)

### ğŸ”„ **Mode DÃ©gradÃ© Automatique**
- **LLM disponible** : Classification zero-shot + rÃ©sumÃ©s IA
- **LLM indisponible** : Fallback automatique vers Expert System
- **Transition transparente** : Aucune interruption de service

---

## ğŸ› ï¸ Maintenance & Monitoring v3.0

### ğŸ“‹ **Monitoring IA Quotidien**
```bash
# âœ… DÃ©marrage rapide avec monitoring LLM intÃ©grÃ©
start_containers.bat

# ğŸ¤– VÃ©rifier statut modÃ¨les IA
curl http://localhost:15000/status

# ğŸ“Š MÃ©triques classification LLM vs fallback
curl http://localhost:15000/config/info

# ğŸ³ Statut conteneurs + ressources LLM
docker ps --filter "name=rss_" && docker stats rss_llm_service --no-stream
```

### ğŸ” **Analyse Performance IA**
```bash
# Analyser distribution classifications LLM vs fallback
grep "classification_method.*llm" obsidian_vault/articles/*/*.md | wc -l
grep "classification_method.*fallback" obsidian_vault/articles/*/*.md | wc -l

# DÃ©tecter articles Ã  confiance faible (rÃ©entraÃ®nement ?)
grep "confidence.*[45][0-9]" obsidian_vault/articles/*/*.md

# Analyser tags IA gÃ©nÃ©rÃ©s
grep "tags.*urgent\|critical" obsidian_vault/articles/*/*.md
```

### ğŸ”„ **Maintenance IA AvancÃ©e**
```bash
# Rechargement configuration Ã  chaud (sans restart)
curl -X POST http://localhost:15000/reload_config

# Restart service LLM uniquement (cache prÃ©servÃ©)
docker restart rss_llm_service

# Nettoyage cache modÃ¨les (si problÃ¨me performance)
docker volume rm rss_llm_cache && deployment\start_pipeline.bat
```

---

## ğŸš€ Workflow Quotidien Optimal v3.0

### ğŸŒ… **Routine Matinale IA (15 secondes)**
1. **Double-clic** : `start_containers.bat`
2. **Chargement automatique** : ModÃ¨les LLM + Expert System
3. **Auto-ouverture** : Node-RED avec monitoring IA
4. **VÃ©rification** : Dashboard articles + tags IA par domaine
5. **IA active** : Classification + rÃ©sumÃ©s automatiques toute la journÃ©e

### ğŸ“Š **Surveillance IA Continue**
- **Articles IA** : GÃ©nÃ©rÃ©s automatiquement toutes les 30min avec tags intelligents
- **Classification** : Monitoring LLM vs fallback dans Node-RED Debug
- **Performance** : Temps de traitement <1s, disponibilitÃ© >99%
- **QualitÃ©** : Confidence LLM >90%, fallback >80%

### ğŸŒ™ **ArrÃªt IA en Fin de JournÃ©e (5 secondes)**
1. **Double-clic** : `stop_containers.bat` 
2. **Sauvegarde** : Cache modÃ¨les LLM prÃ©servÃ©
3. **Ressources** : 12 Go RAM libÃ©rÃ©s proprement

---

## ğŸ¯ Avantages Architecture Hybride v3.0

### ğŸ¤– **Intelligence Artificielle RÃ©elle**
- **Classification zero-shot** : Comprend le contexte, pas seulement des mots-clÃ©s
- **RÃ©sumÃ©s adaptatifs** : GÃ©nÃ©ration contextuelle par domaine de fraude
- **Tags intelligents** : Extraction automatique de concepts pertinents
- **Alertes contextuelles** : DÃ©tection fine des niveaux d'urgence

### ğŸ›¡ï¸ **FiabilitÃ© Expert System**
- **Fallback automatique** : Service toujours disponible mÃªme si LLM en Ã©chec
- **Performance garantie** : <1s mÃªme en mode dÃ©gradÃ©
- **Configuration experte** : 250+ mots-clÃ©s spÃ©cialisÃ©s anti-fraude
- **TraÃ§abilitÃ©** : Logs dÃ©taillÃ©s LLM + fallback

### âš¡ **Performance OptimisÃ©e**
- **Cache intelligent** : ModÃ¨les LLM chargÃ©s une fois, rÃ©utilisÃ©s
- **CPU uniquement** : Pas de dÃ©pendance GPU coÃ»teuse
- **DÃ©marrage rapide** : 15s quotidien vs 10min premiÃ¨re fois
- **Ressources maÃ®trisÃ©es** : 12 Go RAM pour IA complÃ¨te

---

## ğŸ†˜ DÃ©pannage v3.0

### âŒ ProblÃ¨mes IA Courants

**ModÃ¨les LLM ne se chargent pas**
```bash
# VÃ©rifier logs de chargement IA
docker logs rss_llm_service | grep "Loading\|Error"

# Tester endpoint santÃ© IA
curl http://localhost:15000/health

# Forcer re-tÃ©lÃ©chargement modÃ¨les
docker volume rm rss_llm_cache && deployment\start_pipeline.bat
```

**Classification incohÃ©rente LLM vs Expert**
```bash
# Analyser rÃ©partition mÃ©thodes de classification
grep "classification_method" obsidian_vault/articles/*/*.md | sort | uniq -c

# Tester classification manuelle
curl -X POST http://localhost:15000/classify \
  -H "Content-Type: application/json" \
  -d '{"title":"Test article","content":"Content test","source":"Test"}'
```

**Performance IA dÃ©gradÃ©e**
```bash
# VÃ©rifier ressources conteneur LLM
docker stats rss_llm_service --no-stream

# Monitoring temps de traitement
grep "processing_time" obsidian_vault/articles/*/*.md | tail -20

# Restart optimisÃ© (cache prÃ©servÃ©)
docker restart rss_llm_service
```

### ğŸ”§ Reset Complet v3.0
```bash
# Tout supprimer (modÃ¨les + cache + config)
stop_containers.bat
docker system prune -a -f --volumes
deployment\start_pipeline.bat  # Re-tÃ©lÃ©chargement complet
```

---

## ğŸ“ˆ Roadmap v3.0+

### âœ… Version Actuelle (v3.0)
- **IA hybride** : DeBERTa + distilbart + fallback expert intelligent
- **Tags automatiques** : GÃ©nÃ©ration contextuelle en anglais (6 max)
- **Alertes intelligentes** : 4 niveaux avec dÃ©tection contextuelle
- **Performance** : <1s classification + rÃ©sumÃ©, >99% disponibilitÃ©
- **Monitoring avancÃ©** : APIs statut, rechargement Ã  chaud

### ğŸ¯ Prochaines Versions
- [ ] **v3.1** : Interface web gestion modÃ¨les IA + fine-tuning
- [ ] **v3.2** : ModÃ¨les IA spÃ©cialisÃ©s par domaine + apprentissage continu  
- [ ] **v3.3** : IA multilingue (FR/EN/ES) + dÃ©tection langue auto
- [ ] **v4.0** : LLM local personnalisÃ© + pipeline RAG + compliance IA

---

## ğŸ¤ Contribution v3.0

### ğŸ”§ DÃ©veloppement IA Local
```bash
# Fork du repo
git clone [YOUR_FORK]

# DÃ©veloppement avec modÃ¨les IA en local
deployment\start_pipeline.bat

# Tests IA aprÃ¨s modifications
deployment\test_pipeline.bat

# Test endpoints IA manuels
curl -X POST http://localhost:15000/generate_metadata \
  -H "Content-Type: application/json" \
  -d @test_article.json
```

### ğŸ“ Contribution FonctionnalitÃ©s IA
1. **Nouveaux modÃ¨les LLM** : Ajouter dans `llm_service/app.py`
2. **Domaines spÃ©cialisÃ©s** : Enrichir classification zero-shot
3. **Prompts optimisÃ©s** : AmÃ©liorer gÃ©nÃ©ration rÃ©sumÃ©s
4. **MÃ©triques IA** : Ajouter monitoring performance

---

## ğŸ“„ Licence

**MIT License** - Utilisation libre pour projets personnels et commerciaux  
**Note IA** : ModÃ¨les Hugging Face sous licences respectives (Apache 2.0)

---

## ğŸ”— Liens Utiles v3.0

- **Scripts quotidiens IA** : `start_containers.bat` / `stop_containers.bat`
- **Service IA hybride** : http://localhost:15000/status
- **Pipeline Node-RED** : http://localhost:18880
- **Monitoring modÃ¨les** : http://localhost:15000/config/info
- **Configuration** : `config/sources.json` + prompts LLM
- **Articles enrichis IA** : `obsidian_vault/articles/`

**ğŸ‰ Votre Pipeline RSS + IA Hybride v3.0 est prÃªt !**

*DerniÃ¨re mise Ã  jour : 01/06/2025 â€¢ Version 3.0 Architecture Hybride LLM + Expert System*
